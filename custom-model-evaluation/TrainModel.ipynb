{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23d5b7cf-df73-47a2-8d59-16dc4f8d74da",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49a8f492-be80-4437-a0d9-107f0326f6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/gsm8k\"\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e05a65e-1279-4c25-897c-be3a48156f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-us-east-1-419599817388/sagemaker/gsm8k/gsm8k.jsonl\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(\n",
    "    path=\"gsm8k.jsonl\",\n",
    "    bucket=bucket,\n",
    "    key_prefix=prefix\n",
    ")\n",
    "print(\"input spec (in this case, just an S3 path): {}\".format(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a56fd83-163a-4cc3-82df-3881dbde09c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from pathlib import Path\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point='fine_tune.py',\n",
    "    source_dir=f'{Path.cwd()}/src',\n",
    "    role=role,\n",
    "    py_version=\"py311\",\n",
    "    framework_version='2.3.0',\n",
    "    instance_count=1,\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    hyperparameters={\n",
    "        \"epochs\": 10,\n",
    "        \"model-id\": \"unsloth/Llama-3.2-1B-Instruct\",\n",
    "        \"lr\": 1e-3,\n",
    "        \"data-file\": \"gsm8k.jsonl\",\n",
    "    },\n",
    "    disable_output_compression=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a65bfca-e663-4313-9067-464ee514c4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2025-05-16-15-58-34-571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-16 15:58:35 Starting - Starting the training job\n",
      "2025-05-16 15:58:35 Pending - Training job waiting for capacity......\n",
      "2025-05-16 15:59:39 Pending - Preparing the instances for training...\n",
      "2025-05-16 16:00:05 Downloading - Downloading input data...\n",
      "2025-05-16 16:00:30 Downloading - Downloading the training image.....................\n",
      "2025-05-16 16:03:58 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34mCUDA compat package should be installed for NVIDIA driver smaller than 530.30.02\u001b[0m\n",
      "\u001b[34mCurrent installed NVIDIA driver version is 550.163.01\u001b[0m\n",
      "\u001b[34mSkipping CUDA compat setup as newer NVIDIA driver is installed\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m2025-05-16 16:04:14,900 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2025-05-16 16:04:14,919 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-05-16 16:04:14,930 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2025-05-16 16:04:14,932 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2025-05-16 16:04:16,517 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting datasets==3.6.0 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting peft==0.15.2 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting trl==0.11.4 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading trl-0.11.4-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes==0.45.5 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers==0.20.3 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.45.2 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\u001b[0m\n",
      "\u001b[34mCollecting hf_xet==1.1.1 (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading hf_xet-1.1.1-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\u001b[0m\n",
      "\u001b[34mCollecting accelerate==1.7.0 (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets==3.6.0->-r requirements.txt (line 1)) (3.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from datasets==3.6.0->-r requirements.txt (line 1)) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets==3.6.0->-r requirements.txt (line 1)) (19.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets==3.6.0->-r requirements.txt (line 1)) (0.3.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets==3.6.0->-r requirements.txt (line 1)) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.11/site-packages (from datasets==3.6.0->-r requirements.txt (line 1)) (2.32.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.11/site-packages (from datasets==3.6.0->-r requirements.txt (line 1)) (4.66.4)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets==3.6.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.11/site-packages (from datasets==3.6.0->-r requirements.txt (line 1)) (0.70.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 1)) (2024.5.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.24.0 (from datasets==3.6.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.31.2-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from datasets==3.6.0->-r requirements.txt (line 1)) (23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets==3.6.0->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from peft==0.15.2->-r requirements.txt (line 2)) (5.9.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.11/site-packages (from peft==0.15.2->-r requirements.txt (line 2)) (2.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors in /opt/conda/lib/python3.11/site-packages (from peft==0.15.2->-r requirements.txt (line 2)) (0.4.3)\u001b[0m\n",
      "\u001b[34mCollecting tyro>=0.5.11 (from trl==0.11.4->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading tyro-0.9.20-py3-none-any.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers==4.45.2->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.11.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets==3.6.0->-r requirements.txt (line 1)) (4.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets==3.6.0->-r requirements.txt (line 1)) (3.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets==3.6.0->-r requirements.txt (line 1)) (3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets==3.6.0->-r requirements.txt (line 1)) (1.26.20)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets==3.6.0->-r requirements.txt (line 1)) (2025.1.31)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.15.2->-r requirements.txt (line 2)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.15.2->-r requirements.txt (line 2)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.15.2->-r requirements.txt (line 2)) (3.1.6)\u001b[0m\n",
      "\u001b[34mCollecting docstring-parser>=0.15 (from tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.11/site-packages (from tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 3)) (13.7.1)\u001b[0m\n",
      "\u001b[34mCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting typeguard>=4.0.0 (from tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading typeguard-4.4.2-py3-none-any.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting typing-extensions>=3.7.4.3 (from huggingface-hub>=0.24.0->datasets==3.6.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets==3.6.0->-r requirements.txt (line 1)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets==3.6.0->-r requirements.txt (line 1)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets==3.6.0->-r requirements.txt (line 1)) (2024.1)\u001b[0m\n",
      "\u001b[34mCollecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 1)) (23.2.0)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets==3.6.0->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 3)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 3)) (2.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft==0.15.2->-r requirements.txt (line 2)) (2.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.13.0->peft==0.15.2->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 3)) (0.1.2)\u001b[0m\n",
      "\u001b[34mDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\u001b[0m\n",
      "\u001b[34mDownloading peft-0.15.2-py3-none-any.whl (411 kB)\u001b[0m\n",
      "\u001b[34mDownloading trl-0.11.4-py3-none-any.whl (316 kB)\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.1/76.1 MB 100.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 78.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/9.9 MB 107.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading hf_xet-1.1.1-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 25.5/25.5 MB 103.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading accelerate-1.7.0-py3-none-any.whl (362 kB)\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.31.2-py3-none-any.whl (484 kB)\u001b[0m\n",
      "\u001b[34mDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 792.7/792.7 kB 45.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tyro-0.9.20-py3-none-any.whl (125 kB)\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.11.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 80.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\u001b[0m\n",
      "\u001b[34mDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mDownloading typeguard-4.4.2-py3-none-any.whl (35 kB)\u001b[0m\n",
      "\u001b[34mDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\u001b[0m\n",
      "\u001b[34mDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\u001b[0m\n",
      "\u001b[34mDownloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: xxhash, typing-extensions, shtab, regex, propcache, multidict, hf_xet, frozenlist, docstring-parser, aiohappyeyeballs, yarl, typeguard, huggingface-hub, aiosignal, tyro, tokenizers, bitsandbytes, aiohttp, accelerate, transformers, peft, datasets, trl\u001b[0m\n",
      "\u001b[34mAttempting uninstall: typing-extensions\u001b[0m\n",
      "\u001b[34mFound existing installation: typing_extensions 4.11.0\u001b[0m\n",
      "\u001b[34mUninstalling typing_extensions-4.11.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled typing_extensions-4.11.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface_hub 0.23.0\u001b[0m\n",
      "\u001b[34mUninstalling huggingface_hub-0.23.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface_hub-0.23.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.30.1\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.30.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.30.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-1.7.0 aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 bitsandbytes-0.45.5 datasets-3.6.0 docstring-parser-0.16 frozenlist-1.6.0 hf_xet-1.1.1 huggingface-hub-0.31.2 multidict-6.4.3 peft-0.15.2 propcache-0.3.1 regex-2024.11.6 shtab-1.7.2 tokenizers-0.20.3 transformers-4.45.2 trl-0.11.4 typeguard-4.4.2 typing-extensions-4.13.2 tyro-0.9.20 xxhash-3.5.0 yarl-1.20.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 25.0.1 -> 25.1.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2025-05-16 16:04:33,382 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2025-05-16 16:04:33,382 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2025-05-16 16:04:33,421 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-05-16 16:04:33,497 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-05-16 16:04:33,528 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-05-16 16:04:33,540 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"data-file\": \"gsm8k.jsonl\",\n",
      "        \"epochs\": 10,\n",
      "        \"lr\": 0.001,\n",
      "        \"model-id\": \"unsloth/Llama-3.2-1B-Instruct\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2025-05-16-15-58-34-571\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-419599817388/pytorch-training-2025-05-16-15-58-34-571/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"fine_tune\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"fine_tune.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"data-file\":\"gsm8k.jsonl\",\"epochs\":10,\"lr\":0.001,\"model-id\":\"unsloth/Llama-3.2-1B-Instruct\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=fine_tune.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=fine_tune\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-419599817388/pytorch-training-2025-05-16-15-58-34-571/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"data-file\":\"gsm8k.jsonl\",\"epochs\":10,\"lr\":0.001,\"model-id\":\"unsloth/Llama-3.2-1B-Instruct\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"pytorch-training-2025-05-16-15-58-34-571\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-419599817388/pytorch-training-2025-05-16-15-58-34-571/source/sourcedir.tar.gz\",\"module_name\":\"fine_tune\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"fine_tune.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--data-file\",\"gsm8k.jsonl\",\"--epochs\",\"10\",\"--lr\",\"0.001\",\"--model-id\",\"unsloth/Llama-3.2-1B-Instruct\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_DATA-FILE=gsm8k.jsonl\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.001\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL-ID=unsloth/Llama-3.2-1B-Instruct\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python311.zip:/opt/conda/lib/python3.11:/opt/conda/lib/python3.11/lib-dynload:/opt/conda/lib/python3.11/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.11 fine_tune.py --data-file gsm8k.jsonl --epochs 10 --lr 0.001 --model-id unsloth/Llama-3.2-1B-Instruct\u001b[0m\n",
      "\u001b[34m2025-05-16 16:04:33,541 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2025-05-16 16:04:33,542 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10/10 [00:00<00:00, 119.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/10 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 10/10 [00:00<00:00, 1328.95 examples/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/20 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 1/20 [00:00<00:12,  1.52it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 2/20 [00:00<00:06,  2.90it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 3/20 [00:01<00:06,  2.51it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 4/20 [00:01<00:04,  3.36it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 5/20 [00:01<00:05,  2.80it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 6/20 [00:01<00:03,  3.64it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 7/20 [00:02<00:04,  3.08it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 8/20 [00:02<00:03,  3.75it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 9/20 [00:03<00:03,  3.05it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 10/20 [00:03<00:02,  3.84it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 11/20 [00:03<00:02,  3.10it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 12/20 [00:03<00:02,  3.73it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 13/20 [00:04<00:02,  3.06it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 14/20 [00:04<00:01,  3.83it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 15/20 [00:04<00:01,  3.10it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 16/20 [00:04<00:01,  3.84it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 17/20 [00:05<00:00,  3.11it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 18/20 [00:05<00:00,  3.88it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 19/20 [00:05<00:00,  3.13it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 20/20 [00:06<00:00,  3.87it/s]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 6.3663, 'train_samples_per_second': 15.708, 'train_steps_per_second': 3.142, 'train_loss': 1.438272190093994, 'epoch': 10.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 20/20 [00:06<00:00,  3.87it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 20/20 [00:06<00:00,  3.14it/s]\u001b[0m\n",
      "\u001b[34m2025-05-16 16:05:14,052 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2025-05-16 16:05:14,052 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2025-05-16 16:05:14,053 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2025-05-16 16:06:09 Uploading - Uploading generated training model\n",
      "2025-05-16 16:06:42 Completed - Training job completed\n",
      "Training seconds: 396\n",
      "Billable seconds: 396\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({\"training\": inputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2f643e-4a9d-42c8-b421-dadd081f3712",
   "metadata": {},
   "source": [
    "## Move model artifacts for Custom Model Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11a3fd11-3ee1-46e0-a0f6-6e915d65ac06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Artifacts at s3://sagemaker-us-east-1-419599817388/pytorch-training-2025-05-16-15-58-34-571/output/model\n"
     ]
    }
   ],
   "source": [
    "last_train_job = estimator.jobs[-1].describe()\n",
    "artifact_path = last_train_job['ModelArtifacts']['S3ModelArtifacts']\n",
    "artifact_key = artifact_path[artifact_path.find(bucket) + len(bucket):].lstrip('/')\n",
    "print(f'Model Artifacts at {artifact_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e1e78754-9c75-4045-918d-f0e9b6dfb0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import tarfile\n",
    "\n",
    "sts_client = boto3.client('sts')\n",
    "account_info = sts_client.get_caller_identity()\n",
    "account_id = account_info['Account']\n",
    "\n",
    "bucket_name = f\"bedrock-custom-model-{account_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "867ca1d1-d40e-4a1d-96d0-8b7686b49f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy pytorch-training-2025-05-16-15-58-34-571/output/model/config.json -> fine-tuned-model/config.json\n",
      "Copy pytorch-training-2025-05-16-15-58-34-571/output/model/generation_config.json -> fine-tuned-model/generation_config.json\n",
      "Copy pytorch-training-2025-05-16-15-58-34-571/output/model/model-00001-of-00003.safetensors -> fine-tuned-model/model-00001-of-00003.safetensors\n",
      "Copy pytorch-training-2025-05-16-15-58-34-571/output/model/model-00002-of-00003.safetensors -> fine-tuned-model/model-00002-of-00003.safetensors\n",
      "Copy pytorch-training-2025-05-16-15-58-34-571/output/model/model-00003-of-00003.safetensors -> fine-tuned-model/model-00003-of-00003.safetensors\n",
      "Copy pytorch-training-2025-05-16-15-58-34-571/output/model/model.safetensors.index.json -> fine-tuned-model/model.safetensors.index.json\n",
      "Copy pytorch-training-2025-05-16-15-58-34-571/output/model/special_tokens_map.json -> fine-tuned-model/special_tokens_map.json\n",
      "Copy pytorch-training-2025-05-16-15-58-34-571/output/model/tokenizer.json -> fine-tuned-model/tokenizer.json\n",
      "Copy pytorch-training-2025-05-16-15-58-34-571/output/model/tokenizer_config.json -> fine-tuned-model/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "src_bucket = s3.Bucket(bucket)\n",
    "dst_bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "for obj in src_bucket.objects.filter(Prefix=artifact_key):\n",
    "\told_source = {'Bucket': bucket, 'Key': obj.key}\n",
    "\tnew_key = obj.key.replace(artifact_key, 'fine-tuned-model', 1)\n",
    "\tprint(f\"Copy {obj.key} -> {new_key}\")\n",
    "\tnew_obj = dst_bucket.Object(new_key)\n",
    "\tnew_obj.copy(old_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bd367b-b94d-4b1c-9fa7-e6bb8e8ac646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698a4b0d-5c78-4fb1-8d6f-ac75c25b9f55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
